{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([1]) tensor([0.], requires_grad=True)\n",
      "0.0\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ") \n",
      " tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#난수 발생 순서 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#데이터는 텐서의 형태를 가지고 있어야 되고.. 정확한 건 아직 모름\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "print(x_train)\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "#모델 초기화 ( W, b, optimizer) , W,b output tensor shpae이 1 이라는 거임\n",
    "W = torch.zeros(1,requires_grad=True)\n",
    "print(W.shape,W)\n",
    "print(W.item()) #만약 tensor에 하나의 값만 존재한다면, .item() 을 사용하면 숫자 값을 얻을 수 있다.\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W,b],lr=0.01)\n",
    "print(optimizer, '\\n',b)\n",
    "\n",
    "# #가설 세우기\n",
    "# hypothesis = W*x_train+b\n",
    "\n",
    "# #비용함수 정의\n",
    "# cost = torch.mean((y_train-hypothesis)**2)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# cost.backward()\n",
    "# print(x_train.grad)\n",
    "# print(W.grad, b.grad) #cost는 W,b로 미분을 수행한다. 그래서 자동미분시 b,W_grad에 b,W로 미분한 값이 저장된다.\n",
    "# optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 2000\n",
    "\n",
    "for i in range(nb_epoch+1):\n",
    "    \n",
    "    hypothesis = W*x_train+b\n",
    "    \n",
    "    cost = torch.mean((y_train-hypothesis)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #출력문\n",
    "    if i % 100 == 0 :\n",
    "        print(\"epoch:{:4d}/{}  W:{:.3f}  b:{:.3f}  Cost:{:.6f}\".format(i,nb_epoch,W.item(),b.item(),cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0/3000   W: 0.280   b:0.120   Cost:42.000000\n",
      "Epoch:  100/3000   W: 2.619   b:0.867   Cost:0.108384\n",
      "Epoch:  200/3000   W: 2.700   b:0.682   Cost:0.066975\n",
      "Epoch:  300/3000   W: 2.764   b:0.536   Cost:0.041386\n",
      "Epoch:  400/3000   W: 2.815   b:0.421   Cost:0.025574\n",
      "Epoch:  500/3000   W: 2.854   b:0.331   Cost:0.015803\n",
      "Epoch:  600/3000   W: 2.886   b:0.260   Cost:0.009765\n",
      "Epoch:  700/3000   W: 2.910   b:0.205   Cost:0.006034\n",
      "Epoch:  800/3000   W: 2.929   b:0.161   Cost:0.003729\n",
      "Epoch:  900/3000   W: 2.944   b:0.126   Cost:0.002304\n",
      "Epoch: 1000/3000   W: 2.956   b:0.099   Cost:0.001424\n",
      "Epoch: 1100/3000   W: 2.966   b:0.078   Cost:0.000880\n",
      "Epoch: 1200/3000   W: 2.973   b:0.061   Cost:0.000544\n",
      "Epoch: 1300/3000   W: 2.979   b:0.048   Cost:0.000336\n",
      "Epoch: 1400/3000   W: 2.983   b:0.038   Cost:0.000208\n",
      "Epoch: 1500/3000   W: 2.987   b:0.030   Cost:0.000128\n",
      "Epoch: 1600/3000   W: 2.990   b:0.023   Cost:0.000079\n",
      "Epoch: 1700/3000   W: 2.992   b:0.018   Cost:0.000049\n",
      "Epoch: 1800/3000   W: 2.994   b:0.014   Cost:0.000030\n",
      "Epoch: 1900/3000   W: 2.995   b:0.011   Cost:0.000019\n",
      "Epoch: 2000/3000   W: 2.996   b:0.009   Cost:0.000012\n",
      "Epoch: 2100/3000   W: 2.997   b:0.007   Cost:0.000007\n",
      "Epoch: 2200/3000   W: 2.998   b:0.006   Cost:0.000004\n",
      "Epoch: 2300/3000   W: 2.998   b:0.004   Cost:0.000003\n",
      "Epoch: 2400/3000   W: 2.998   b:0.003   Cost:0.000002\n",
      "Epoch: 2500/3000   W: 2.999   b:0.003   Cost:0.000001\n",
      "Epoch: 2600/3000   W: 2.999   b:0.002   Cost:0.000001\n",
      "Epoch: 2700/3000   W: 2.999   b:0.002   Cost:0.000000\n",
      "Epoch: 2800/3000   W: 2.999   b:0.001   Cost:0.000000\n",
      "Epoch: 2900/3000   W: 3.000   b:0.001   Cost:0.000000\n",
      "Epoch: 3000/3000   W: 3.000   b:0.001   Cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "#데이터\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[3],[6],[9]])\n",
    "\n",
    "#모델 초기화 : Gradient Descent\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "optimizer = optim.SGD([W,b],lr=0.01)\n",
    "\n",
    "\n",
    "nb_epochs = 3000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "#가설 : Linear Equation\n",
    "    hypothesis = W*x_train+b\n",
    "#cost function : MSE\n",
    "    cost = torch.mean((y_train-hypothesis)**2)\n",
    "#경사하강\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "#결과 확인\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: {:4d}/{}   W: {:.3f}   b:{:.3f}   Cost:{:.6f}\"\n",
    "              .format(epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable Linear regression Practice\n",
    "\n",
    "$H(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/1000  w1 : 0.294   w2 : 0.294   w3 : 0.290   b : 0.003  Cost : 29661.800781\n",
      "Epoch :  100/1000  w1 : 0.688   w2 : 0.681   w3 : 0.658   b : 0.008  Cost : 5.754573\n",
      "Epoch :  200/1000  w1 : 0.697   w2 : 0.684   w3 : 0.645   b : 0.008  Cost : 5.512386\n",
      "Epoch :  300/1000  w1 : 0.707   w2 : 0.686   w3 : 0.634   b : 0.008  Cost : 5.281667\n",
      "Epoch :  400/1000  w1 : 0.715   w2 : 0.689   w3 : 0.622   b : 0.008  Cost : 5.061869\n",
      "Epoch :  500/1000  w1 : 0.724   w2 : 0.691   w3 : 0.611   b : 0.008  Cost : 4.852424\n",
      "Epoch :  600/1000  w1 : 0.733   w2 : 0.693   w3 : 0.600   b : 0.008  Cost : 4.652706\n",
      "Epoch :  700/1000  w1 : 0.741   w2 : 0.695   w3 : 0.589   b : 0.009  Cost : 4.462287\n",
      "Epoch :  800/1000  w1 : 0.750   w2 : 0.697   w3 : 0.579   b : 0.009  Cost : 4.280604\n",
      "Epoch :  900/1000  w1 : 0.758   w2 : 0.699   w3 : 0.569   b : 0.009  Cost : 4.107294\n",
      "Epoch : 1000/1000  w1 : 0.766   w2 : 0.700   w3 : 0.559   b : 0.009  Cost : 3.941866\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#데이터\n",
    "x1_train = torch.FloatTensor([[73],[93],[89],[96],[73]])\n",
    "x2_train = torch.FloatTensor([[80],[88],[91],[98],[66]])\n",
    "x3_train = torch.FloatTensor([[75],[93],[80],[100],[70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "#model initialization (w, b, optimizer)\n",
    "w1 = torch.zeros(1, requires_grad =True)\n",
    "w2 = torch.zeros(1, requires_grad =True)\n",
    "w3 = torch.zeros(1, requires_grad =True)\n",
    "b = torch.zeros(1, requires_grad =True)\n",
    "\n",
    "optimizer = optim.SGD([w1,w2,w3,b], lr = 1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "#epoch num\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #가설\n",
    "    hypothesis = x1_train*w1+x2_train*w2+x3_train*w3+b\n",
    "    \n",
    "    #cost function(MSE)\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    #ccst로 H(x) 개선 \n",
    "    optimizer.zero_grad() #기울기 초기화\n",
    "    cost.backward() #cost 미분\n",
    "    optimizer.step() #W , b  업데이트\n",
    "    \n",
    "    if epoch % 100  == 0 :\n",
    "        print('Epoch : {:4d}/{}  w1 : {:.3f}   w2 : {:.3f}   w3 : {:.3f}   b : {:.3f}  Cost : {:.6f}'.format(\n",
    "        epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 개선모델 1 : x, y가 무수히 늘어나면 안되니까,  Dot Product 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0/20   Hypothesis : tensor([0., 0., 0., 0., 0.])   Cost : 29661.800781\n",
      "Epoch :  1/20   Hypothesis : tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605])   Cost : 9298.520508\n",
      "Epoch :  2/20   Hypothesis : tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821])   Cost : 2915.712402\n",
      "Epoch :  3/20   Hypothesis : tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097])   Cost : 915.040527\n",
      "Epoch :  4/20   Hypothesis : tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307])   Cost : 287.936096\n",
      "Epoch :  5/20   Hypothesis : tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891])   Cost : 91.371071\n",
      "Epoch :  6/20   Hypothesis : tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812])   Cost : 29.758249\n",
      "Epoch :  7/20   Hypothesis : tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805])   Cost : 10.445267\n",
      "Epoch :  8/20   Hypothesis : tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440])   Cost : 4.391237\n",
      "Epoch :  9/20   Hypothesis : tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396])   Cost : 2.493121\n",
      "Epoch : 10/20   Hypothesis : tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732])   Cost : 1.897688\n",
      "Epoch : 11/20   Hypothesis : tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602])   Cost : 1.710552\n",
      "Epoch : 12/20   Hypothesis : tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651])   Cost : 1.651416\n",
      "Epoch : 13/20   Hypothesis : tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240])   Cost : 1.632369\n",
      "Epoch : 14/20   Hypothesis : tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571])   Cost : 1.625924\n",
      "Epoch : 15/20   Hypothesis : tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759])   Cost : 1.623420\n",
      "Epoch : 16/20   Hypothesis : tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865])   Cost : 1.622152\n",
      "Epoch : 17/20   Hypothesis : tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927])   Cost : 1.621261\n",
      "Epoch : 18/20   Hypothesis : tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963])   Cost : 1.620501\n",
      "Epoch : 19/20   Hypothesis : tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985])   Cost : 1.619757\n",
      "Epoch : 20/20   Hypothesis : tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999])   Cost : 1.619046\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "#model\n",
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "optimizer = optim.SGD([W,b],lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #hypothesis : linear Equation\n",
    "    hypothesis = x_train.matmul(W)+b\n",
    "    \n",
    "    #cost function : MSE\n",
    "    cost = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    #cost 로 H(x) 최적화 : optimizer Algorithm ( Gradient Descent )\n",
    "    optimizer.zero_grad() #기울기 0으로 초기화\n",
    "    cost.backward() #cost 를 w, b에 대해 미분하여 기울기를 구하여  w_grad에저장 . b는 우에대지 일단 넘어가자미\n",
    "    optimizer.step() # 학습률 * 기울기 곱해서 W에 빼줌\n",
    "    \n",
    "    print('Epoch : {:2d}/{}   Hypothesis : {}   Cost : {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))#hypothesis 그냥 출력하면 (1,5)  + grad_fn까지 같이 나옴 squeeze로 \n",
    "#     (5,) 만들어 주고, detach로 grad_fn(Tensor생성한 function)분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "True\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<AddBackward0>)\n",
      "False\n",
      "tensor(True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]])\n"
     ]
    }
   ],
   "source": [
    "#  .grad_fn 속성, 이는 Tensor 를 생성한 Function 을 참조\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n",
    "\n",
    "#x, y는 내용물은 같지만, y는 requires_grad = False로 기울기가 기록되지 않음\n",
    "print(x.requires_grad)\n",
    "y = x + 1\n",
    "print(y)\n",
    "\n",
    "#x에 어떤 연산을 해서 넣는지에 따라서 바뀌어서 좀 들어간다. 먼말이지 암튼 grad_fn이랑 requires_grad랑 좀 다름.\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())\n",
    "\n",
    "#.detach() 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지\n",
    "print(x)\n",
    "print(x.detach())\n",
    "\n",
    "print(z)\n",
    "print(z.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개선모델 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/2000    Cost : 31667.597656\n",
      "Epoch :  100/2000    Cost : 0.225988\n",
      "Epoch :  200/2000    Cost : 0.223910\n",
      "Epoch :  300/2000    Cost : 0.221930\n",
      "Epoch :  400/2000    Cost : 0.220059\n",
      "Epoch :  500/2000    Cost : 0.218271\n",
      "Epoch :  600/2000    Cost : 0.216572\n",
      "Epoch :  700/2000    Cost : 0.214955\n",
      "Epoch :  800/2000    Cost : 0.213415\n",
      "Epoch :  900/2000    Cost : 0.211952\n",
      "Epoch : 1000/2000    Cost : 0.210554\n",
      "Epoch : 1100/2000    Cost : 0.209230\n",
      "Epoch : 1200/2000    Cost : 0.207966\n",
      "Epoch : 1300/2000    Cost : 0.206768\n",
      "Epoch : 1400/2000    Cost : 0.205618\n",
      "Epoch : 1500/2000    Cost : 0.204526\n",
      "Epoch : 1600/2000    Cost : 0.203479\n",
      "Epoch : 1700/2000    Cost : 0.202486\n",
      "Epoch : 1800/2000    Cost : 0.201539\n",
      "Epoch : 1900/2000    Cost : 0.200637\n",
      "Epoch : 2000/2000    Cost : 0.199769\n",
      "\n",
      "\n",
      " tensor([[151.2305]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#setting\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1) # 필수임 안하면 계속 바뀜\n",
    "\n",
    "#data\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "#model : Linear combination\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #H(x) 가설, 예측 값 함수 선언\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    #cost function nn.mocdule에 있음.\n",
    "    cost = F.mse_loss(prediction,y_train)\n",
    "    \n",
    "    #gradient Descent 해주나..? ? 아니지 이건 따로지\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print( 'Epoch : {:4d}/{}    Cost : {:.6f}'.format(\n",
    "                epoch, nb_epochs, cost.item()))\n",
    "\n",
    "new_var = torch.FloatTensor([[73, 80, 75]])\n",
    "pred_y = model(new_var)\n",
    "\n",
    "print(\"\\n\\n\" ,pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 클래스로 단순 선형 회귀, 다중 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#data\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "#Class Declaration\n",
    "class LinealRegressionModel(nn.Module): #class는 소문자임. ㄷ걍 대부분 예약어 소문자임!\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1) # data 보고 결정\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x) #model(x) 와 model.forward()는 똑같음.\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])    \n",
    "    \n",
    "class MultivariableLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini batch Training with Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0/20   Batch: 0/3   Cost: 35424.417969\n",
      "Epoch :  0/20   Batch: 1/3   Cost: 7990.055664\n",
      "Epoch :  0/20   Batch: 2/3   Cost: 2148.731934\n",
      "Epoch :  1/20   Batch: 0/3   Cost: 986.442322\n",
      "Epoch :  1/20   Batch: 1/3   Cost: 465.397644\n",
      "Epoch :  1/20   Batch: 2/3   Cost: 88.113068\n",
      "Epoch :  2/20   Batch: 0/3   Cost: 36.668221\n",
      "Epoch :  2/20   Batch: 1/3   Cost: 9.448990\n",
      "Epoch :  2/20   Batch: 2/3   Cost: 3.667141\n",
      "Epoch :  3/20   Batch: 0/3   Cost: 0.975254\n",
      "Epoch :  3/20   Batch: 1/3   Cost: 0.906193\n",
      "Epoch :  3/20   Batch: 2/3   Cost: 0.006388\n",
      "Epoch :  4/20   Batch: 0/3   Cost: 0.051697\n",
      "Epoch :  4/20   Batch: 1/3   Cost: 0.054978\n",
      "Epoch :  4/20   Batch: 2/3   Cost: 1.199195\n",
      "Epoch :  5/20   Batch: 0/3   Cost: 0.401478\n",
      "Epoch :  5/20   Batch: 1/3   Cost: 0.046481\n",
      "Epoch :  5/20   Batch: 2/3   Cost: 0.388473\n",
      "Epoch :  6/20   Batch: 0/3   Cost: 0.538637\n",
      "Epoch :  6/20   Batch: 1/3   Cost: 0.145426\n",
      "Epoch :  6/20   Batch: 2/3   Cost: 0.053031\n",
      "Epoch :  7/20   Batch: 0/3   Cost: 0.601826\n",
      "Epoch :  7/20   Batch: 1/3   Cost: 0.188046\n",
      "Epoch :  7/20   Batch: 2/3   Cost: 0.048246\n",
      "Epoch :  8/20   Batch: 0/3   Cost: 0.044538\n",
      "Epoch :  8/20   Batch: 1/3   Cost: 0.509270\n",
      "Epoch :  8/20   Batch: 2/3   Cost: 0.279848\n",
      "Epoch :  9/20   Batch: 0/3   Cost: 0.051684\n",
      "Epoch :  9/20   Batch: 1/3   Cost: 0.079211\n",
      "Epoch :  9/20   Batch: 2/3   Cost: 1.242455\n",
      "Epoch : 10/20   Batch: 0/3   Cost: 0.039547\n",
      "Epoch : 10/20   Batch: 1/3   Cost: 0.284169\n",
      "Epoch : 10/20   Batch: 2/3   Cost: 1.030069\n",
      "Epoch : 11/20   Batch: 0/3   Cost: 0.402872\n",
      "Epoch : 11/20   Batch: 1/3   Cost: 0.285181\n",
      "Epoch : 11/20   Batch: 2/3   Cost: 0.017461\n",
      "Epoch : 12/20   Batch: 0/3   Cost: 0.045682\n",
      "Epoch : 12/20   Batch: 1/3   Cost: 0.093299\n",
      "Epoch : 12/20   Batch: 2/3   Cost: 1.044303\n",
      "Epoch : 13/20   Batch: 0/3   Cost: 0.401862\n",
      "Epoch : 13/20   Batch: 1/3   Cost: 0.055938\n",
      "Epoch : 13/20   Batch: 2/3   Cost: 0.412103\n",
      "Epoch : 14/20   Batch: 0/3   Cost: 0.537061\n",
      "Epoch : 14/20   Batch: 1/3   Cost: 0.010415\n",
      "Epoch : 14/20   Batch: 2/3   Cost: 0.253107\n",
      "Epoch : 15/20   Batch: 0/3   Cost: 0.625113\n",
      "Epoch : 15/20   Batch: 1/3   Cost: 0.015669\n",
      "Epoch : 15/20   Batch: 2/3   Cost: 0.070435\n",
      "Epoch : 16/20   Batch: 0/3   Cost: 0.551100\n",
      "Epoch : 16/20   Batch: 1/3   Cost: 0.126779\n",
      "Epoch : 16/20   Batch: 2/3   Cost: 0.001243\n",
      "Epoch : 17/20   Batch: 0/3   Cost: 0.074117\n",
      "Epoch : 17/20   Batch: 1/3   Cost: 0.049969\n",
      "Epoch : 17/20   Batch: 2/3   Cost: 1.040596\n",
      "Epoch : 18/20   Batch: 0/3   Cost: 0.064074\n",
      "Epoch : 18/20   Batch: 1/3   Cost: 0.334032\n",
      "Epoch : 18/20   Batch: 2/3   Cost: 0.990896\n",
      "Epoch : 19/20   Batch: 0/3   Cost: 0.354361\n",
      "Epoch : 19/20   Batch: 1/3   Cost: 0.372612\n",
      "Epoch : 19/20   Batch: 2/3   Cost: 0.308847\n",
      "Epoch : 20/20   Batch: 0/3   Cost: 0.041474\n",
      "Epoch : 20/20   Batch: 1/3   Cost: 0.466467\n",
      "Epoch : 20/20   Batch: 2/3   Cost: 0.352104\n"
     ]
    }
   ],
   "source": [
    "#Setting\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#data \n",
    "x_train = torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "#model, optimizer\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "#epoch num\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        \n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        #H(x)\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        #cost function : MSE\n",
    "        cost = F.mse_loss(prediction,y_train)\n",
    "        \n",
    "        #cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch : {:2d}/{}   Batch: {}/{}   Cost: {:.6f}'.format(epoch, nb_epochs, batch_idx, len(dataloader), cost.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/20   Batch: 0/5   Cost: 25215.304688\n",
      "Epoch :    0/20   Batch: 1/5   Cost: 15332.123047\n",
      "Epoch :    0/20   Batch: 2/5   Cost: 3631.480225\n",
      "Epoch :    0/20   Batch: 3/5   Cost: 1153.570190\n",
      "Epoch :    0/20   Batch: 4/5   Cost: 108.236061\n",
      "Epoch :    1/20   Batch: 0/5   Cost: 79.761940\n",
      "Epoch :    1/20   Batch: 1/5   Cost: 37.894341\n",
      "Epoch :    1/20   Batch: 2/5   Cost: 5.665077\n",
      "Epoch :    1/20   Batch: 3/5   Cost: 2.459033\n",
      "Epoch :    1/20   Batch: 4/5   Cost: 0.417272\n",
      "Epoch :    2/20   Batch: 0/5   Cost: 1.922523\n",
      "Epoch :    2/20   Batch: 1/5   Cost: 0.054261\n",
      "Epoch :    2/20   Batch: 2/5   Cost: 0.284908\n",
      "Epoch :    2/20   Batch: 3/5   Cost: 0.003929\n",
      "Epoch :    2/20   Batch: 4/5   Cost: 0.024053\n",
      "Epoch :    3/20   Batch: 0/5   Cost: 1.012643\n",
      "Epoch :    3/20   Batch: 1/5   Cost: 0.004206\n",
      "Epoch :    3/20   Batch: 2/5   Cost: 0.463283\n",
      "Epoch :    3/20   Batch: 3/5   Cost: 0.020964\n",
      "Epoch :    3/20   Batch: 4/5   Cost: 0.017084\n",
      "Epoch :    4/20   Batch: 0/5   Cost: 0.973626\n",
      "Epoch :    4/20   Batch: 1/5   Cost: 0.006318\n",
      "Epoch :    4/20   Batch: 2/5   Cost: 0.473706\n",
      "Epoch :    4/20   Batch: 3/5   Cost: 0.022193\n",
      "Epoch :    4/20   Batch: 4/5   Cost: 0.016850\n",
      "Epoch :    5/20   Batch: 0/5   Cost: 0.970797\n",
      "Epoch :    5/20   Batch: 1/5   Cost: 0.006378\n",
      "Epoch :    5/20   Batch: 2/5   Cost: 0.474483\n",
      "Epoch :    5/20   Batch: 3/5   Cost: 0.022261\n",
      "Epoch :    5/20   Batch: 4/5   Cost: 0.016921\n",
      "Epoch :    6/20   Batch: 0/5   Cost: 0.969745\n",
      "Epoch :    6/20   Batch: 1/5   Cost: 0.006325\n",
      "Epoch :    6/20   Batch: 2/5   Cost: 0.474799\n",
      "Epoch :    6/20   Batch: 3/5   Cost: 0.022270\n",
      "Epoch :    6/20   Batch: 4/5   Cost: 0.017013\n",
      "Epoch :    7/20   Batch: 0/5   Cost: 0.968784\n",
      "Epoch :    7/20   Batch: 1/5   Cost: 0.006264\n",
      "Epoch :    7/20   Batch: 2/5   Cost: 0.475093\n",
      "Epoch :    7/20   Batch: 3/5   Cost: 0.022279\n",
      "Epoch :    7/20   Batch: 4/5   Cost: 0.017096\n",
      "Epoch :    8/20   Batch: 0/5   Cost: 0.967853\n",
      "Epoch :    8/20   Batch: 1/5   Cost: 0.006209\n",
      "Epoch :    8/20   Batch: 2/5   Cost: 0.475367\n",
      "Epoch :    8/20   Batch: 3/5   Cost: 0.022288\n",
      "Epoch :    8/20   Batch: 4/5   Cost: 0.017188\n",
      "Epoch :    9/20   Batch: 0/5   Cost: 0.966922\n",
      "Epoch :    9/20   Batch: 1/5   Cost: 0.006149\n",
      "Epoch :    9/20   Batch: 2/5   Cost: 0.475661\n",
      "Epoch :    9/20   Batch: 3/5   Cost: 0.022293\n",
      "Epoch :    9/20   Batch: 4/5   Cost: 0.017276\n",
      "Epoch :   10/20   Batch: 0/5   Cost: 0.965962\n",
      "Epoch :   10/20   Batch: 1/5   Cost: 0.006094\n",
      "Epoch :   10/20   Batch: 2/5   Cost: 0.475956\n",
      "Epoch :   10/20   Batch: 3/5   Cost: 0.022302\n",
      "Epoch :   10/20   Batch: 4/5   Cost: 0.017373\n",
      "Epoch :   11/20   Batch: 0/5   Cost: 0.965003\n",
      "Epoch :   11/20   Batch: 1/5   Cost: 0.006032\n",
      "Epoch :   11/20   Batch: 2/5   Cost: 0.476230\n",
      "Epoch :   11/20   Batch: 3/5   Cost: 0.022311\n",
      "Epoch :   11/20   Batch: 4/5   Cost: 0.017457\n",
      "Epoch :   12/20   Batch: 0/5   Cost: 0.964104\n",
      "Epoch :   12/20   Batch: 1/5   Cost: 0.005973\n",
      "Epoch :   12/20   Batch: 2/5   Cost: 0.476525\n",
      "Epoch :   12/20   Batch: 3/5   Cost: 0.022315\n",
      "Epoch :   12/20   Batch: 4/5   Cost: 0.017546\n",
      "Epoch :   13/20   Batch: 0/5   Cost: 0.963145\n",
      "Epoch :   13/20   Batch: 1/5   Cost: 0.005919\n",
      "Epoch :   13/20   Batch: 2/5   Cost: 0.476820\n",
      "Epoch :   13/20   Batch: 3/5   Cost: 0.022329\n",
      "Epoch :   13/20   Batch: 4/5   Cost: 0.017635\n",
      "Epoch :   14/20   Batch: 0/5   Cost: 0.962247\n",
      "Epoch :   14/20   Batch: 1/5   Cost: 0.005863\n",
      "Epoch :   14/20   Batch: 2/5   Cost: 0.477115\n",
      "Epoch :   14/20   Batch: 3/5   Cost: 0.022334\n",
      "Epoch :   14/20   Batch: 4/5   Cost: 0.017724\n",
      "Epoch :   15/20   Batch: 0/5   Cost: 0.961289\n",
      "Epoch :   15/20   Batch: 1/5   Cost: 0.005804\n",
      "Epoch :   15/20   Batch: 2/5   Cost: 0.477389\n",
      "Epoch :   15/20   Batch: 3/5   Cost: 0.022343\n",
      "Epoch :   15/20   Batch: 4/5   Cost: 0.017814\n",
      "Epoch :   16/20   Batch: 0/5   Cost: 0.960392\n",
      "Epoch :   16/20   Batch: 1/5   Cost: 0.005751\n",
      "Epoch :   16/20   Batch: 2/5   Cost: 0.477663\n",
      "Epoch :   16/20   Batch: 3/5   Cost: 0.022343\n",
      "Epoch :   16/20   Batch: 4/5   Cost: 0.017908\n",
      "Epoch :   17/20   Batch: 0/5   Cost: 0.959465\n",
      "Epoch :   17/20   Batch: 1/5   Cost: 0.005696\n",
      "Epoch :   17/20   Batch: 2/5   Cost: 0.477937\n",
      "Epoch :   17/20   Batch: 3/5   Cost: 0.022361\n",
      "Epoch :   17/20   Batch: 4/5   Cost: 0.017998\n",
      "Epoch :   18/20   Batch: 0/5   Cost: 0.958538\n",
      "Epoch :   18/20   Batch: 1/5   Cost: 0.005643\n",
      "Epoch :   18/20   Batch: 2/5   Cost: 0.478254\n",
      "Epoch :   18/20   Batch: 3/5   Cost: 0.022366\n",
      "Epoch :   18/20   Batch: 4/5   Cost: 0.018084\n",
      "Epoch :   19/20   Batch: 0/5   Cost: 0.957582\n",
      "Epoch :   19/20   Batch: 1/5   Cost: 0.005586\n",
      "Epoch :   19/20   Batch: 2/5   Cost: 0.478507\n",
      "Epoch :   19/20   Batch: 3/5   Cost: 0.022375\n",
      "Epoch :   19/20   Batch: 4/5   Cost: 0.018174\n",
      "Epoch :   20/20   Batch: 0/5   Cost: 0.956717\n",
      "Epoch :   20/20   Batch: 1/5   Cost: 0.005527\n",
      "Epoch :   20/20   Batch: 2/5   Cost: 0.478803\n",
      "Epoch :   20/20   Batch: 3/5   Cost: 0.022379\n",
      "Epoch :   20/20   Batch: 4/5   Cost: 0.018265\n"
     ]
    }
   ],
   "source": [
    "#setting\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class Customdataset(Dataset):\n",
    "    def __init__(self):\n",
    "         self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "         self.y_data = [[152], [185], [180], [196], [142]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x,y\n",
    "    \n",
    "dataset = Customdataset()\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "\n",
    "#model, optimizer\n",
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "#epoch\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataset):\n",
    "        \n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        #H(x) : Linear Equation\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        #Cost : MSE\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        #cost로 h(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch : {:4d}/{}   Batch: {}/{}   Cost: {:.6f}'.format(epoch, nb_epochs, batch_idx, len(dataset), cost.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151.0224]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#훈련 후 데이터 입력\n",
    "new_var = torch.FloatTensor([[73,80,75]])\n",
    "pred_y = model(new_var)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
